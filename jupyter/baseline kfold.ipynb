{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import wandb\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 데이터 로드"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train_dir = '../../input/data/train'\n",
    "test_dir = '../../input/data/eval'\n",
    "save_dir = '../saved/models/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 하이퍼파라미터"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model_name = 'efficientnet_b1'\n",
    "learning_rate = 1e-5\n",
    "batch_size = 16\n",
    "step_size = 5\n",
    "epochs = 3\n",
    "earlystop = 5\n",
    "\n",
    "A_transform = {\n",
    "    'train':\n",
    "        A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            #A.RandomCrop(384, 384),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Cutout(num_holes=8, max_h_size=32,max_w_size=32),\n",
    "            A.ElasticTransform(),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "    'valid':\n",
    "        A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "    'test':\n",
    "        A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class LoadCSV():\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "        self.img_dir =train_dir + '/new_images/'\n",
    "        self.origin_csv_path = train_dir + '/train.csv'\n",
    "        self.trans_csv_path = train_dir + '/trans_train.csv'\n",
    "        \n",
    "        if not os.path.exists(self.trans_csv_path):\n",
    "            self._makeCSV()\n",
    "        self.df = pd.read_csv(self.trans_csv_path)\n",
    "        self.df = self.df[:200]\n",
    "    def _makeCSV(self):        \n",
    "        with open(self.trans_csv_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"path\", \"label\"])\n",
    "\n",
    "            df = pd.read_csv(self.origin_csv_path)\n",
    "            for idx in range(len(df)):\n",
    "                data = df.iloc[idx]\n",
    "                img_path_base = os.path.join(os.path.join(self.img_dir, data['path']), '*')\n",
    "                for img_path in glob.glob(img_path_base):\n",
    "                    label = 0\n",
    "                    if \"incorrect\" in img_path:\n",
    "                        label+=6\n",
    "                    elif 'normal' in img_path:\n",
    "                        label+=12\n",
    "                    elif data['gender']=='female':\n",
    "                        label+=3\n",
    "                    elif data['age'] >= 30 and data['age'] < 60:\n",
    "                        label+=1\n",
    "                    elif data['age'] >= 60:\n",
    "                        label+=2\n",
    "                    writer.writerow([img_path, label])\n",
    "        f.close()\n",
    "\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        super().__init__()\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_id = torch.tensor(self.df['label'].iloc[idx])\n",
    "        img = PIL.Image.open(self.df['path'].iloc[idx])\n",
    "        img = np.array(img.convert(\"RGB\"))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, class_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 모델 설계\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = torch.nn.Linear(in_features=n_features, out_features=num_classes, bias=True)\n",
    "        # n_features = self.model.head.in_features\n",
    "        # self.model.head = torch.nn.Linear(in_features=n_features, out_features=self.num_classes, bias=True)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.model.classifier.weight)\n",
    "        stdv = 1/np.sqrt(self.num_classes)\n",
    "        self.model.classifier.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MyModel(model_name, 18).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 학습"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "today = time.strftime('%Y%m%d_%H%M%S', time.localtime(time.time()))\n",
    "if not os.path.exists(save_dir + today):\n",
    "    os.makedirs(save_dir + today)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "earlystop_value = 0\n",
    "best_model = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0\n",
    "best_loss = 999999999\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "mask_csv = LoadCSV(train_dir)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kfold.split(mask_csv.df['path'], mask_csv.df['label'])):\n",
    "    print(f'FOLD {fold}')\n",
    "    mask_train = MaskDataset(mask_csv.df,  transform=A_transform['train'])\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = DataLoader(mask_train, batch_size=batch_size, sampler=train_subsampler, drop_last=False, num_workers=8, pin_memory=True)\n",
    "    valid_loader = DataLoader(mask_train, batch_size=batch_size, sampler=valid_subsampler, drop_last=False, num_workers=8, pin_memory=True)\n",
    "    dataloaders = {'train': train_loader, 'valid':valid_loader}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if earlystop_value >= earlystop:\n",
    "            break\n",
    "        train_loss, valid_loss, train_acc_list, valid_acc_list = 0, 0, [],[]\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            with tqdm(dataloaders[phase], total=dataloaders[phase].__len__(), unit=\"batch\") as train_bar:\n",
    "                for inputs, labels in train_bar:\n",
    "                    train_bar.set_description(f\"{phase} Epoch {epoch} \")\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    outputs = outputs.cpu().detach().numpy()\n",
    "                    labels = labels.cpu().detach().numpy()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += (np.argmax(outputs, axis=1)== labels).mean()\n",
    "                    epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "                    epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "                    train_bar.set_postfix(loss=epoch_loss, acc=epoch_acc)\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            if phase=='valid':\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save(best_model_wts, f'{save_dir}{today}/baseline_{model_name}_lr{learning_rate}_stepLR{step_size}_batch{batch_size}_epoch{epoch}_valid_loss_{epoch_loss:.5f}.pt')\n",
    "                    earlystop_value = 0\n",
    "                else:\n",
    "                    earlystop_value += 1\n",
    "\n",
    "    model.load_state_dict(best_model_wts)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?batch/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOLD 0\n",
      "[ 18  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
      "  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75\n",
      "  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93\n",
      "  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111\n",
      " 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129\n",
      " 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147\n",
      " 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165\n",
      " 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183\n",
      " 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train Epoch 0 : 100%|██████████| 10/10 [00:02<00:00,  3.98batch/s, acc=0.00344, loss=2.51]\n",
      "valid Epoch 0 : 100%|██████████| 3/3 [00:01<00:00,  2.48batch/s, acc=0.00125, loss=0.631]\n",
      "train Epoch 1 : 100%|██████████| 10/10 [00:01<00:00,  5.02batch/s, acc=0.00313, loss=2.48]\n",
      "valid Epoch 1 : 100%|██████████| 3/3 [00:01<00:00,  2.71batch/s, acc=0.00156, loss=0.612]\n",
      "train Epoch 2 : 100%|██████████| 10/10 [00:02<00:00,  4.49batch/s, acc=0.00469, loss=2.39]\n",
      "valid Epoch 2 : 100%|██████████| 3/3 [00:01<00:00,  2.42batch/s, acc=0.0025, loss=0.563]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOLD 1\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  66  67  76  77  78  79  80  81  83  89  90  91  92  93\n",
      "  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111\n",
      " 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129\n",
      " 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147\n",
      " 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165\n",
      " 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183\n",
      " 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199] [18 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63\n",
      " 64 65 68 69 70 71 72 73 74 75 82 84 85 86 87 88]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train Epoch 0 : 100%|██████████| 10/10 [00:02<00:00,  4.59batch/s, acc=0.00531, loss=2.31]\n",
      "valid Epoch 0 : 100%|██████████| 3/3 [00:01<00:00,  2.88batch/s, acc=0.00281, loss=0.569]\n",
      "train Epoch 1 : 100%|██████████| 10/10 [00:02<00:00,  4.57batch/s, acc=0.00781, loss=2.25]\n",
      "valid Epoch 1 : 100%|██████████| 3/3 [00:01<00:00,  2.34batch/s, acc=0.00281, loss=0.567]\n",
      "train Epoch 2 : 100%|██████████| 10/10 [00:01<00:00,  5.03batch/s, acc=0.00813, loss=2.21]\n",
      "valid Epoch 2 : 100%|██████████| 3/3 [00:01<00:00,  2.33batch/s, acc=0.00219, loss=0.556]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOLD 2\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  68  69  70  71  72  73\n",
      "  74  75  79  80  81  82  84  85  86  87  88 118 124 125 126 127 128 129\n",
      " 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147\n",
      " 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165\n",
      " 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183\n",
      " 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199] [ 66  67  76  77  78  83  89  90  91  92  93  94  95  96  97  98  99 100\n",
      " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 119\n",
      " 120 121 122 123]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train Epoch 0 : 100%|██████████| 10/10 [00:02<00:00,  4.90batch/s, acc=0.00562, loss=2.2]\n",
      "valid Epoch 0 : 100%|██████████| 3/3 [00:01<00:00,  2.38batch/s, acc=0.00219, loss=0.527]\n",
      "train Epoch 1 : 100%|██████████| 10/10 [00:02<00:00,  4.08batch/s, acc=0.0109, loss=2.09]\n",
      "valid Epoch 1 : 100%|██████████| 3/3 [00:01<00:00,  2.53batch/s, acc=0.00375, loss=0.501]\n",
      "train Epoch 2 : 100%|██████████| 10/10 [00:02<00:00,  4.67batch/s, acc=0.0116, loss=2.01]\n",
      "valid Epoch 2 : 100%|██████████| 3/3 [00:01<00:00,  2.56batch/s, acc=0.00344, loss=0.492]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOLD 3\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  82  83  84  85  86  87  88  89  90  91  92\n",
      "  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110\n",
      " 111 112 113 114 115 116 117 119 120 121 122 123 159 160 161 162 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 183\n",
      " 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199] [ 79  80  81 118 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 156 157 158 182]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train Epoch 0 : 100%|██████████| 10/10 [00:02<00:00,  4.46batch/s, acc=0.0122, loss=1.98]\n",
      "valid Epoch 0 : 100%|██████████| 3/3 [00:01<00:00,  2.49batch/s, acc=0.0025, loss=0.485]\n",
      "train Epoch 1 : 100%|██████████| 10/10 [00:02<00:00,  4.63batch/s, acc=0.0147, loss=1.9]\n",
      "valid Epoch 1 : 100%|██████████| 3/3 [00:01<00:00,  2.92batch/s, acc=0.00344, loss=0.491]\n",
      "train Epoch 2 : 100%|██████████| 10/10 [00:02<00:00,  4.69batch/s, acc=0.0197, loss=1.8]\n",
      "valid Epoch 2 : 100%|██████████| 3/3 [00:01<00:00,  2.61batch/s, acc=0.00469, loss=0.476]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOLD 4\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 182] [159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176\n",
      " 177 178 179 180 181 183 184 185 186 187 188 189 190 191 192 193 194 195\n",
      " 196 197 198 199]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train Epoch 0 : 100%|██████████| 10/10 [00:01<00:00,  5.17batch/s, acc=0.0187, loss=1.81]\n",
      "valid Epoch 0 : 100%|██████████| 3/3 [00:01<00:00,  2.62batch/s, acc=0.00594, loss=0.461]\n",
      "train Epoch 1 : 100%|██████████| 10/10 [00:02<00:00,  4.72batch/s, acc=0.0153, loss=1.86]\n",
      "valid Epoch 1 : 100%|██████████| 3/3 [00:01<00:00,  2.98batch/s, acc=0.00719, loss=0.422]\n",
      "train Epoch 2 : 100%|██████████| 10/10 [00:01<00:00,  5.01batch/s, acc=0.0169, loss=1.84]\n",
      "valid Epoch 2 : 100%|██████████| 3/3 [00:01<00:00,  2.63batch/s, acc=0.00781, loss=0.421]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. 추론"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = PIL.Image.open(self.img_paths[index])\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)\n",
    "            image = image['image']\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'new_images')\n",
    "\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "dataset = TestDataset(image_paths, A_transform['test'])\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "with tqdm(test_loader, total=test_loader.__len__(), unit=\"batch\") as test_bar:\n",
    "    for images in test_bar:\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            pred = model(images)\n",
    "            pred = pred.argmax(dim=-1)\n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "    \n",
    "submission['ans'] = all_predictions\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 6/788 [00:00<00:57, 13.52batch/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../input/data/eval/new_images/d8c0d7ae6cf662506012135e730b855c321dbc8f.jpg'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f7e02160ce8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mall_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtest_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a18ed2e941b8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2903\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2904\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2905\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../input/data/eval/new_images/d8c0d7ae6cf662506012135e730b855c321dbc8f.jpg'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "with wandb.init(project=model_name + str(today), entity='nudago'):\n",
    "    wandb_config = wandb.config\n",
    "    wandb_config.learning_rate = 0.01\n",
    "\n",
    "    wandb.log({\"loss\": loss})\n",
    "# wandb.log({\"loss:\" loss.item()}, step=example_ct)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnudago\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">breezy-snowflake-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nudago/vit_base_patch16_38420210831_034340\" target=\"_blank\">https://wandb.ai/nudago/vit_base_patch16_38420210831_034340</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nudago/vit_base_patch16_38420210831_034340/runs/3vwh66jt\" target=\"_blank\">https://wandb.ai/nudago/vit_base_patch16_38420210831_034340/runs/3vwh66jt</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/image-classification-level1-04/jupyter/wandb/run-20210831_034340-3vwh66jt</code><br/><br/>\n",
       "            "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21991<br/>Program ended successfully."
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4d25e92135d43eba84d8ab2b020535d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find user logs for this run at: <code>/opt/ml/image-classification-level1-04/jupyter/wandb/run-20210831_034340-3vwh66jt/logs/debug.log</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find internal logs for this run at: <code>/opt/ml/image-classification-level1-04/jupyter/wandb/run-20210831_034340-3vwh66jt/logs/debug-internal.log</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">breezy-snowflake-1</strong>: <a href=\"https://wandb.ai/nudago/vit_base_patch16_38420210831_034340/runs/3vwh66jt\" target=\"_blank\">https://wandb.ai/nudago/vit_base_patch16_38420210831_034340/runs/3vwh66jt</a><br/>\n",
       "                "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}